apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  dataDirHostPath: /var/lib/rook
  cephVersion:
    image: quay.io/ceph/ceph:v17.1.0-20220317
  mon:
    count: 3
    allowMultiplePerNode: false
    volumeClaimTemplate:
      spec:
        storageClassName: gp2
        resources:
          requests:
            storage: 60Gi
  storage:
    storageClassDeviceSets:
      - name: gp2
        count: 6 # 6
        portable: true # allow OSDs to move between nodes when there are failures
        tuneDeviceClass: true # gp2 provides relatively slow devices
        volumeClaimTemplates:
          - metadata:
              name: data
            spec:
              resources:
                requests:
                  storage: 100Gi # 100Gi
              storageClassName: gp2
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
        placement:
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
        preparePlacement:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd
                      - key: app
                        operator: In
                        values:
                          - rook-ceph-osd-prepare
                  topologyKey: kubernetes.io/hostname
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: DoNotSchedule
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd-prepare
  priorityClassNames:
    # critical storage components should get high system priority. because mons/osds can be
    # rescheduled to different nodes, use system-cluster-critical instead of system-node-critical
    mon: system-cluster-critical
    osd: system-cluster-critical
    mgr: system-cluster-critical
  resources:
    # mem+cpu requests+limits that are equal allow a pod to have "Guaranteed" QoS class
    mon:
      requests:
        cpu: "1"
        memory: "2Gi"
      limits:
        cpu: "1"
        memory: "2Gi"
    osd:
      requests:
        cpu: "2"
        memory: "5Gi"
      limits:
        cpu: "2"
        memory: "5Gi"
    mgr:
      requests:
        cpu: "1"
        memory: "3Gi"
      limits:
        cpu: "1"
        memory: "3Gi"
    mgr-sidecar:
      requests:
        cpu: "1"
        memory: "40Mi"
      limits:
        cpu: "1"
        memory: "100Mi"
    # prepareosd: # leaving this blank means k8s won't kill OSD provisioning due to resource usage
